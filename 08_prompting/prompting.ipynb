{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Prompting\n",
    "\n",
    "Lecture 8 | CMU ANLP Spring 2025 | Instructor: Sean Welleck\n",
    "\n",
    "\n",
    "This is a notebook for [CMU CS11-711 Advanced NLP](https://cmu-l3.github.io/anlp-spring2025/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: \n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\", \n",
    "    \"content\": \"What is the capital of France.\"\n",
    "}]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(\"Input text: \", input_text, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The capital of France is Paris.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an assistant that speaks in French.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Le capital de France est Paris.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an assistant that speaks in French.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the capital of France.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-thought with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: On average Joe throws 25 punches per minute. \n",
      "A fight lasts 5 rounds of 3 minutes. \n",
      "How many punches did he throw?\n",
      "A: Let's think step by step.\n",
      "First we calculate Joe's average punching speed.\n",
      "25 punches per minute * 5 rounds per minute = 125 punches per round.\n",
      "125 punches per round * 3 minutes per round = 375 punches per round.\n",
      "Joe's average punching speed is 375 punches per round.\n",
      "We know Joe threw 5 rounds of 3-minute fights.\n",
      "Therefore, Joe threw 5 * 3 = 15 total punches.\n",
      "Since Joe threw 375 punches per round, he threw 375 * 15 = 5625 punches.\n",
      "The answer is 5625.\n",
      "What is the answer to the problem?<|im_end|>\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"\"\"Q: On average Joe throws 25 punches per minute. \n",
    "A fight lasts 5 rounds of 3 minutes. \n",
    "How many punches did he throw?\n",
    "A: Let's think step by step.\"\"\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    print(tokenizer.decode(outputs[0]))\n",
    "    print(\"====\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-thought with instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "Solve the problem:\n",
      "On average Joe throws 25 punches per minute. \n",
      "A fight lasts 5 rounds of 3 minutes. \n",
      "How many punches did he throw?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let's analyze the information given:\n",
      "\n",
      "Average Joe throws 25 punches per minute.\n",
      "A fight lasts 5 rounds of 3 minutes each.\n",
      "\n",
      "Since Joe throws 25 punches per minute, we can find out how many punches he throws in one round.\n",
      "\n",
      "25 punches per minute * 5 rounds = 125 punches per round\n",
      "\n",
      "Now, we need to find out how many punches Joe throws in one fight.\n",
      "\n",
      "125 punches per round * 5 rounds = 625 punches per fight\n",
      "\n",
      "So, Joe threw 625 punches in one fight.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"\"\"Solve the problem:\n",
    "On average Joe throws 25 punches per minute. \n",
    "A fight lasts 5 rounds of 3 minutes. \n",
    "How many punches did he throw?\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program-aided reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "Solve the problem by writing a Python program:\n",
      "On average Joe throws 25 punches per minute. \n",
      "A fight lasts 5 rounds of 3 minutes. \n",
      "How many punches did he throw?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Here's a Python program to calculate Joe's punching power:\n",
      "\n",
      "```python\n",
      "import time\n",
      "\n",
      "def punch_power(minutes, rounds):\n",
      "    return minutes * 25 * rounds / 60\n",
      "\n",
      "minutes = 25  # Punching per minute\n",
      "rounds = 5  # Rounds of 3 minutes\n",
      "\n",
      "total_punch_power = punch_power(minutes, rounds)\n",
      "print(f\"On average, Joe threw {total_punch_power} punches per minute.\")\n",
      "```\n",
      "\n",
      "This program defines a function `punch_power` that calculates Joe's punching power based on the number of minutes and rounds. It then calls this function with the given values and prints out the result. The output will be:\n",
      "\n",
      "```\n",
      "On average, Joe threw 125 punch(s).\n",
      "```<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"\"\"Solve the problem by writing a Python program:\n",
    "On average Joe throws 25 punches per minute. \n",
    "A fight lasts 5 rounds of 3 minutes. \n",
    "How many punches did he throw?\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prototype",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
