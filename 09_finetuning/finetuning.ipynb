{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Fine-tuning\n",
    "\n",
    "Lecture 9 | CMU ANLP Spring 2025 | Instructor: Sean Welleck\n",
    "\n",
    "\n",
    "This is a notebook for [CMU CS11-711 Advanced NLP](https://cmu-l3.github.io/anlp-spring2025/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paityn',\n",
       " 'evalyn',\n",
       " 'luz',\n",
       " 'nathalia',\n",
       " 'winnie',\n",
       " 'chandler',\n",
       " 'ciara',\n",
       " 'danica',\n",
       " 'nailah',\n",
       " 'rilynn']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('names.txt').read().splitlines()\n",
    "data[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model)\n",
    "\n",
    "# Add a new pad token to the tokenizer\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\": \"<|pad|>\",\n",
    "    \"bos_token\": \"<|startoftext|>\",\n",
    "})\n",
    "\n",
    "# Add the new pad token to the model\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, names, tokenizer, max_length=128):\n",
    "        self.names = names\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.tokenizer.bos_token + self.names[idx] + self.tokenizer.eos_token\n",
    "        return name\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # pad based on the longest sequence in the batch using padding \"longest\"\n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            padding=\"longest\", \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True\n",
    "        )\n",
    "        return inputs\n",
    "\n",
    "\n",
    "# Split into train, dev, test\n",
    "import random\n",
    "random.seed(123)\n",
    "random.shuffle(data)\n",
    "\n",
    "n1 = int(0.8 * len(data))\n",
    "n2 = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n1]\n",
    "dev_data = data[n1:n2]\n",
    "test_data = data[n2:]\n",
    "\n",
    "# Create the datasets and dataloaders\n",
    "train_dataset = NamesDataset(train_data, tokenizer)\n",
    "dev_dataset = NamesDataset(dev_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False, collate_fn=dev_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49153,    90,  2214,  7650,     0],\n",
       "        [49153,  4933,   693,   254,     0],\n",
       "        [49153,  5802,   552,    81,     0],\n",
       "        [49153, 14151, 10333,     0, 49152],\n",
       "        [49153, 10328,   373,  5488,     0],\n",
       "        [49153,  6925,   792,    95,     0],\n",
       "        [49153,   541,   552,    85,     0],\n",
       "        [49153,    92,  1111,  1287,     0],\n",
       "        [49153, 38093, 18133,     0, 49152],\n",
       "        [49153,    91,  9331,     0, 49152],\n",
       "        [49153,    88,  1131,   332,     0],\n",
       "        [49153,    99, 45896,     0, 49152],\n",
       "        [49153,  1317, 15034,     0, 49152],\n",
       "        [49153,   763,   105,   391,     0],\n",
       "        [49153, 21900,   992,     0, 49152],\n",
       "        [49153,   264, 10675,     0, 49152]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example batch\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftext|>marilla<|endoftext|><|pad|>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([49153,  3298,  7712,     0, 49152])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 134516160\n",
      "Epoch [1/1], Step [0/1602], Loss: 10.6860\n",
      "Epoch [1/1], Step [10/1602], Loss: 7.4223\n",
      "Epoch [1/1], Step [20/1602], Loss: 6.8610\n",
      "Epoch [1/1], Step [30/1602], Loss: 6.5487\n",
      "Epoch [1/1], Step [40/1602], Loss: 6.2980\n",
      "Epoch [1/1], Step [50/1602], Loss: 6.0415\n",
      "Epoch [1/1], Step [60/1602], Loss: 5.8234\n",
      "Epoch [1/1], Step [70/1602], Loss: 5.5985\n",
      "Epoch [1/1], Step [80/1602], Loss: 5.3880\n",
      "Epoch [1/1], Step [90/1602], Loss: 5.1745\n",
      "Epoch [1/1], Step [100/1602], Loss: 4.9964\n",
      "Epoch [1/1], Step [110/1602], Loss: 4.8135\n",
      "Epoch [1/1], Step [120/1602], Loss: 4.6661\n",
      "Epoch [1/1], Step [130/1602], Loss: 4.5397\n",
      "Epoch [1/1], Step [140/1602], Loss: 4.4444\n",
      "Epoch [1/1], Step [150/1602], Loss: 4.3554\n",
      "Epoch [1/1], Step [160/1602], Loss: 4.2737\n",
      "Epoch [1/1], Step [170/1602], Loss: 4.1982\n",
      "Epoch [1/1], Step [180/1602], Loss: 4.1369\n",
      "Epoch [1/1], Step [190/1602], Loss: 4.0737\n",
      "Epoch [1/1], Step [200/1602], Loss: 4.0226\n",
      "Epoch [1/1], Step [210/1602], Loss: 3.9771\n",
      "Epoch [1/1], Step [220/1602], Loss: 3.9327\n",
      "Epoch [1/1], Step [230/1602], Loss: 3.8875\n",
      "Epoch [1/1], Step [240/1602], Loss: 3.8484\n",
      "Epoch [1/1], Step [250/1602], Loss: 3.8213\n",
      "Epoch [1/1], Step [260/1602], Loss: 3.7907\n",
      "Epoch [1/1], Step [270/1602], Loss: 3.7562\n",
      "Epoch [1/1], Step [280/1602], Loss: 3.7314\n",
      "Epoch [1/1], Step [290/1602], Loss: 3.7072\n",
      "Epoch [1/1], Step [300/1602], Loss: 3.6843\n",
      "Epoch [1/1], Step [310/1602], Loss: 3.6660\n",
      "Epoch [1/1], Step [320/1602], Loss: 3.6494\n",
      "Epoch [1/1], Step [330/1602], Loss: 3.6282\n",
      "Epoch [1/1], Step [340/1602], Loss: 3.6086\n",
      "Epoch [1/1], Step [350/1602], Loss: 3.5927\n",
      "Epoch [1/1], Step [360/1602], Loss: 3.5811\n",
      "Epoch [1/1], Step [370/1602], Loss: 3.5672\n",
      "Epoch [1/1], Step [380/1602], Loss: 3.5535\n",
      "Epoch [1/1], Step [390/1602], Loss: 3.5367\n",
      "Epoch [1/1], Step [400/1602], Loss: 3.5215\n",
      "Epoch [1/1], Step [410/1602], Loss: 3.5065\n",
      "Epoch [1/1], Step [420/1602], Loss: 3.4956\n",
      "Epoch [1/1], Step [430/1602], Loss: 3.4809\n",
      "Epoch [1/1], Step [440/1602], Loss: 3.4712\n",
      "Epoch [1/1], Step [450/1602], Loss: 3.4596\n",
      "Epoch [1/1], Step [460/1602], Loss: 3.4483\n",
      "Epoch [1/1], Step [470/1602], Loss: 3.4401\n",
      "Epoch [1/1], Step [480/1602], Loss: 3.4304\n",
      "Epoch [1/1], Step [490/1602], Loss: 3.4212\n",
      "Epoch [1/1], Step [500/1602], Loss: 3.4100\n",
      "Epoch [1/1], Step [510/1602], Loss: 3.4022\n",
      "Epoch [1/1], Step [520/1602], Loss: 3.3965\n",
      "Epoch [1/1], Step [530/1602], Loss: 3.3881\n",
      "Epoch [1/1], Step [540/1602], Loss: 3.3816\n",
      "Epoch [1/1], Step [550/1602], Loss: 3.3760\n",
      "Epoch [1/1], Step [560/1602], Loss: 3.3677\n",
      "Epoch [1/1], Step [570/1602], Loss: 3.3596\n",
      "Epoch [1/1], Step [580/1602], Loss: 3.3516\n",
      "Epoch [1/1], Step [590/1602], Loss: 3.3435\n",
      "Epoch [1/1], Step [600/1602], Loss: 3.3351\n",
      "Epoch [1/1], Step [610/1602], Loss: 3.3269\n",
      "Epoch [1/1], Step [620/1602], Loss: 3.3205\n",
      "Epoch [1/1], Step [630/1602], Loss: 3.3110\n",
      "Epoch [1/1], Step [640/1602], Loss: 3.3061\n",
      "Epoch [1/1], Step [650/1602], Loss: 3.3026\n",
      "Epoch [1/1], Step [660/1602], Loss: 3.2970\n",
      "Epoch [1/1], Step [670/1602], Loss: 3.2918\n",
      "Epoch [1/1], Step [680/1602], Loss: 3.2885\n",
      "Epoch [1/1], Step [690/1602], Loss: 3.2839\n",
      "Epoch [1/1], Step [700/1602], Loss: 3.2773\n",
      "Epoch [1/1], Step [710/1602], Loss: 3.2725\n",
      "Epoch [1/1], Step [720/1602], Loss: 3.2674\n",
      "Epoch [1/1], Step [730/1602], Loss: 3.2662\n",
      "Epoch [1/1], Step [740/1602], Loss: 3.2643\n",
      "Epoch [1/1], Step [750/1602], Loss: 3.2604\n",
      "Epoch [1/1], Step [760/1602], Loss: 3.2552\n",
      "Epoch [1/1], Step [770/1602], Loss: 3.2494\n",
      "Epoch [1/1], Step [780/1602], Loss: 3.2460\n",
      "Epoch [1/1], Step [790/1602], Loss: 3.2410\n",
      "Epoch [1/1], Step [800/1602], Loss: 3.2362\n",
      "Epoch [1/1], Step [810/1602], Loss: 3.2338\n",
      "Epoch [1/1], Step [820/1602], Loss: 3.2290\n",
      "Epoch [1/1], Step [830/1602], Loss: 3.2245\n",
      "Epoch [1/1], Step [840/1602], Loss: 3.2227\n",
      "Epoch [1/1], Step [850/1602], Loss: 3.2190\n",
      "Epoch [1/1], Step [860/1602], Loss: 3.2168\n",
      "Epoch [1/1], Step [870/1602], Loss: 3.2138\n",
      "Epoch [1/1], Step [880/1602], Loss: 3.2089\n",
      "Epoch [1/1], Step [890/1602], Loss: 3.2058\n",
      "Epoch [1/1], Step [900/1602], Loss: 3.2026\n",
      "Epoch [1/1], Step [910/1602], Loss: 3.1997\n",
      "Epoch [1/1], Step [920/1602], Loss: 3.1950\n",
      "Epoch [1/1], Step [930/1602], Loss: 3.1919\n",
      "Epoch [1/1], Step [940/1602], Loss: 3.1890\n",
      "Epoch [1/1], Step [950/1602], Loss: 3.1854\n",
      "Epoch [1/1], Step [960/1602], Loss: 3.1829\n",
      "Epoch [1/1], Step [970/1602], Loss: 3.1797\n",
      "Epoch [1/1], Step [980/1602], Loss: 3.1760\n",
      "Epoch [1/1], Step [990/1602], Loss: 3.1736\n",
      "Epoch [1/1], Step [1000/1602], Loss: 3.1703\n",
      "Epoch [1/1], Step [1010/1602], Loss: 3.1682\n",
      "Epoch [1/1], Step [1020/1602], Loss: 3.1663\n",
      "Epoch [1/1], Step [1030/1602], Loss: 3.1618\n",
      "Epoch [1/1], Step [1040/1602], Loss: 3.1583\n",
      "Epoch [1/1], Step [1050/1602], Loss: 3.1561\n",
      "Epoch [1/1], Step [1060/1602], Loss: 3.1529\n",
      "Epoch [1/1], Step [1070/1602], Loss: 3.1519\n",
      "Epoch [1/1], Step [1080/1602], Loss: 3.1509\n",
      "Epoch [1/1], Step [1090/1602], Loss: 3.1477\n",
      "Epoch [1/1], Step [1100/1602], Loss: 3.1443\n",
      "Epoch [1/1], Step [1110/1602], Loss: 3.1441\n",
      "Epoch [1/1], Step [1120/1602], Loss: 3.1404\n",
      "Epoch [1/1], Step [1130/1602], Loss: 3.1385\n",
      "Epoch [1/1], Step [1140/1602], Loss: 3.1352\n",
      "Epoch [1/1], Step [1150/1602], Loss: 3.1322\n",
      "Epoch [1/1], Step [1160/1602], Loss: 3.1303\n",
      "Epoch [1/1], Step [1170/1602], Loss: 3.1273\n",
      "Epoch [1/1], Step [1180/1602], Loss: 3.1247\n",
      "Epoch [1/1], Step [1190/1602], Loss: 3.1222\n",
      "Epoch [1/1], Step [1200/1602], Loss: 3.1195\n",
      "Epoch [1/1], Step [1210/1602], Loss: 3.1175\n",
      "Epoch [1/1], Step [1220/1602], Loss: 3.1162\n",
      "Epoch [1/1], Step [1230/1602], Loss: 3.1142\n",
      "Epoch [1/1], Step [1240/1602], Loss: 3.1123\n",
      "Epoch [1/1], Step [1250/1602], Loss: 3.1102\n",
      "Epoch [1/1], Step [1260/1602], Loss: 3.1094\n",
      "Epoch [1/1], Step [1270/1602], Loss: 3.1071\n",
      "Epoch [1/1], Step [1280/1602], Loss: 3.1060\n",
      "Epoch [1/1], Step [1290/1602], Loss: 3.1033\n",
      "Epoch [1/1], Step [1300/1602], Loss: 3.1021\n",
      "Epoch [1/1], Step [1310/1602], Loss: 3.1002\n",
      "Epoch [1/1], Step [1320/1602], Loss: 3.0990\n",
      "Epoch [1/1], Step [1330/1602], Loss: 3.0984\n",
      "Epoch [1/1], Step [1340/1602], Loss: 3.0977\n",
      "Epoch [1/1], Step [1350/1602], Loss: 3.0957\n",
      "Epoch [1/1], Step [1360/1602], Loss: 3.0936\n",
      "Epoch [1/1], Step [1370/1602], Loss: 3.0925\n",
      "Epoch [1/1], Step [1380/1602], Loss: 3.0905\n",
      "Epoch [1/1], Step [1390/1602], Loss: 3.0902\n",
      "Epoch [1/1], Step [1400/1602], Loss: 3.0871\n",
      "Epoch [1/1], Step [1410/1602], Loss: 3.0859\n",
      "Epoch [1/1], Step [1420/1602], Loss: 3.0852\n",
      "Epoch [1/1], Step [1430/1602], Loss: 3.0834\n",
      "Epoch [1/1], Step [1440/1602], Loss: 3.0814\n",
      "Epoch [1/1], Step [1450/1602], Loss: 3.0794\n",
      "Epoch [1/1], Step [1460/1602], Loss: 3.0780\n",
      "Epoch [1/1], Step [1470/1602], Loss: 3.0757\n",
      "Epoch [1/1], Step [1480/1602], Loss: 3.0737\n",
      "Epoch [1/1], Step [1490/1602], Loss: 3.0710\n",
      "Epoch [1/1], Step [1500/1602], Loss: 3.0689\n",
      "Epoch [1/1], Step [1510/1602], Loss: 3.0678\n",
      "Epoch [1/1], Step [1520/1602], Loss: 3.0661\n",
      "Epoch [1/1], Step [1530/1602], Loss: 3.0648\n",
      "Epoch [1/1], Step [1540/1602], Loss: 3.0630\n",
      "Epoch [1/1], Step [1550/1602], Loss: 3.0622\n",
      "Epoch [1/1], Step [1560/1602], Loss: 3.0614\n",
      "Epoch [1/1], Step [1570/1602], Loss: 3.0604\n",
      "Epoch [1/1], Step [1580/1602], Loss: 3.0583\n",
      "Epoch [1/1], Step [1590/1602], Loss: 3.0581\n",
      "Epoch [1/1], Step [1600/1602], Loss: 3.0573\n",
      "Epoch [1/1], Loss: 3.0571\n",
      "Epoch [1/1], Validation Loss: 3.0528\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Count model parameters\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate schedule (cosine with warmup)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * num_epochs, eta_min=0)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print average loss every 10 batches\n",
    "        if i % 10 == 0:\n",
    "            avg_loss = total_loss / (i + 1)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Evaluate validation loss\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(dev_loader):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(dev_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_eval_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: chyna\n",
      "Sample 2: cody\n",
      "Sample 3: jayn\n",
      "Sample 4: sabin\n",
      "Sample 5: amruth\n",
      "Sample 6: shari\n",
      "Sample 7: kaylie\n",
      "Sample 8: maelynn\n",
      "Sample 9: kyran\n",
      "Sample 10: zamarion\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"<|startoftext|>\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(\n",
    "    input_ids[\"input_ids\"], \n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    "    max_length=20, \n",
    "    num_return_sequences=10, \n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "for i, sample_output in enumerate(output):\n",
    "    print(f\"Sample {i+1}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prototype",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
